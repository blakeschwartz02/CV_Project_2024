{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill these file paths with the locations on your machine. \n",
    "PATH_TO_CODE = 'C:/Users/blake/OneDrive/Desktop/Computer Vision/Project/pastis-benchmark/code' # path to the code folder of the repo\n",
    "PATH_TO_PASTIS = 'D:/PASTIS'\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(PATH_TO_CODE)\n",
    "\n",
    "# import the necessary packages\n",
    "from torch.nn import ConvTranspose2d\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import Module\n",
    "from torch.nn import ModuleList\n",
    "from torch.nn import ReLU\n",
    "from torchvision.transforms import CenterCrop\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchnet as tnt\n",
    "\n",
    "from torchmetrics import JaccardIndex\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pickle as pkl\n",
    "import time\n",
    "\n",
    "cm = matplotlib.colormaps.get_cmap('tab20')\n",
    "def_colors = cm.colors\n",
    "cus_colors = ['k'] + [def_colors[i] for i in range(1,20)]+['w']\n",
    "cmap = ListedColormap(colors = cus_colors, name='agri',N=21)\n",
    "\n",
    "def get_rgb(x, batch_index=0, t_show=1):\n",
    "    \"\"\"Utility function to get a displayable rgb image \n",
    "    from a Sentinel-2 time series.\n",
    "    \"\"\"\n",
    "    im = x['S2'][batch_index, t_show, [2,1,0]].cuda().numpy()\n",
    "    mx = im.max(axis=(1,2))\n",
    "    mi = im.min(axis=(1,2))   \n",
    "    im = (im - mi[:,None,None])/(mx - mi)[:,None,None]\n",
    "    im = im.swapaxes(0,2).swapaxes(0,1)\n",
    "    im = np.clip(im, a_max=1, a_min=0)\n",
    "    return im\n",
    "\n",
    "def get_radar(x, batch_index=0, t_show=6, orbit='D'):\n",
    "    \"\"\"Utility function to get a displayable image \n",
    "    from a Sentinel-1 time series.\n",
    "    \"\"\"\n",
    "    im = x['S1{}'.format(orbit)][batch_index, t_show].cuda().numpy()\n",
    "    mx = im.max(axis=(1,2))\n",
    "    mi = im.min(axis=(1,2))   \n",
    "    im = (im - mi[:,None,None])/(mx - mi)[:,None,None]\n",
    "    im = im.swapaxes(0,2).swapaxes(0,1)\n",
    "    im = np.clip(im, a_max=1, a_min=0)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading patch metadata . . .\n",
      "Done.\n",
      "Dataset ready.\n"
     ]
    }
   ],
   "source": [
    "from dataloader import PASTIS_Dataset\n",
    "from collate import pad_collate\n",
    "\n",
    "dt = PASTIS_Dataset(PATH_TO_PASTIS, norm=True, target='semantic')\n",
    "# If you only need to evaluate semantic segmentation use target='semantic'\n",
    "\n",
    "dl = torch.utils.data.DataLoader(dt, batch_size=100, collate_fn=pad_collate, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(input_image, input_mask):\n",
    "   if tf.random.uniform(()) > 0.5:\n",
    "       # Random flipping of the image and mask\n",
    "       input_image = tf.image.flip_left_right(input_image)\n",
    "       input_mask = tf.image.flip_left_right(input_mask)\n",
    "\n",
    "   return input_image, input_mask\n",
    "\n",
    "def normalize(input_image, input_mask):\n",
    "   input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "   input_mask -= 1\n",
    "   return input_image, input_mask\n",
    "\n",
    "def load_image_train(x, y):\n",
    "   input_image = x\n",
    "   input_mask = y\n",
    "   input_image, input_mask = augment(input_image, input_mask)\n",
    "   input_image, input_mask = normalize(input_image, input_mask)\n",
    "   return input_image, input_mask\n",
    "\n",
    "def load_image_test(x, y):\n",
    "   input_image = x\n",
    "   input_mask = y\n",
    "   input_image, input_mask = normalize(input_image, input_mask)\n",
    "   return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_todevice(x, device):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.to(device)\n",
    "    elif isinstance(x, dict):\n",
    "        return {k: recursive_todevice(v, device) for k, v in x.items()}\n",
    "    else:\n",
    "        return [recursive_todevice(c, device) for c in x]\n",
    "\n",
    "\n",
    "def prepare_output():\n",
    "    os.makedirs('output', exist_ok=True)\n",
    "    for fold in range(1, 6):\n",
    "        os.makedirs(os.path.join(\"Fold_{}\".format(fold)), exist_ok=True)\n",
    "\n",
    "\n",
    "def checkpoint(fold, log):\n",
    "    with open(\n",
    "        os.path.join('output', \"Fold_{}\".format(fold), \"trainlog.json\"), \"w\"\n",
    "    ) as outfile:\n",
    "        json.dump(log, outfile, indent=4)\n",
    "\n",
    "\n",
    "def save_results(fold, metrics):\n",
    "    if isinstance(metrics, torch.Tensor):\n",
    "        metrics = metrics.cpu().numpy()\n",
    "    with open(\n",
    "        os.path.join('output', \"Fold_{}\".format(fold), \"test_metrics.json\"), \"w\"\n",
    "    ) as outfile:\n",
    "        json.dump(metrics, outfile, indent=4)\n",
    "    \n",
    "def iterate(\n",
    "    model, data_loader, criterion, optimizer=None, mode=\"train\", device=None\n",
    "):\n",
    "    loss_meter = tnt.meter.AverageValueMeter()\n",
    "    IoU = JaccardIndex(task='multiclass', num_classes=20).to(device)\n",
    "    t_start = time.time()\n",
    "\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        if device is not None:\n",
    "            batch = recursive_todevice(batch, device)\n",
    "        (x, dates), y = batch\n",
    "        y = y.long()\n",
    "        x = x['S2'][:, 0, range(0,3), :, :].to(device)\n",
    "\n",
    "        if mode != \"train\":\n",
    "            with torch.no_grad():\n",
    "                out = model(x)\n",
    "        else:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "\n",
    "        loss = criterion(out, y)\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = out.argmax(dim=1)\n",
    "            pred = pred.resize(200, 128, 128)\n",
    "\n",
    "        loss_meter.add(loss.item())\n",
    "\n",
    "        if (i + 1) % 2 == 0:\n",
    "            acc = IoU(pred, y)\n",
    "            print(\n",
    "                \"Step [{}/{}], Loss: {:.4f}, mIoU {:.2f}\".format(\n",
    "                    i + 1, len(data_loader), loss_meter.value()[0], acc\n",
    "                )\n",
    "            )\n",
    "\n",
    "    t_end = time.time()\n",
    "    total_time = t_end - t_start\n",
    "    print(\"Epoch time : {:.1f}s\".format(total_time))\n",
    "    metrics = {\n",
    "        \"{}_loss\".format(mode): loss_meter.value()[0],\n",
    "        \"{}_IoU\".format(mode): acc.item(),\n",
    "        \"{}_epoch_time\".format(mode): total_time,\n",
    "    }\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(Module):\n",
    "\tdef __init__(self, inChannels, outChannels):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# store the convolution and RELU layers\n",
    "\t\tself.conv1 = Conv2d(inChannels, outChannels, 3)\n",
    "\t\tself.relu = ReLU()\n",
    "\t\tself.conv2 = Conv2d(outChannels, outChannels, 3)\n",
    "\tdef forward(self, x):\n",
    "\t\t# apply CONV => RELU => CONV block to the inputs and return it\n",
    "\t\treturn self.conv2(self.relu(self.conv1(x)))\n",
    "\t\n",
    "class Encoder(Module):\n",
    "\tdef __init__(self, channels=(3, 16, 32, 64)):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# store the encoder blocks and maxpooling layer\n",
    "\t\tself.encBlocks = ModuleList(\n",
    "\t\t\t[Block(channels[i], channels[i + 1])\n",
    "\t\t\t \tfor i in range(len(channels) - 1)])\n",
    "\t\tself.pool = MaxPool2d(2)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\t# initialize an empty list to store the intermediate outputs\n",
    "\t\tblockOutputs = []\n",
    "\t\t# loop through the encoder blocks\n",
    "\t\tfor block in self.encBlocks:\n",
    "\t\t\t# pass the inputs through the current encoder block, store\n",
    "\t\t\t# the outputs, and then apply maxpooling on the output\n",
    "\t\t\tx = block(x)\n",
    "\t\t\tblockOutputs.append(x)\n",
    "\t\t\tx = self.pool(x)\n",
    "\t\t# return the list containing the intermediate outputs\n",
    "\t\treturn blockOutputs\n",
    "\t\n",
    "class Decoder(Module):\n",
    "\tdef __init__(self, channels=(64, 32, 16)):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# initialize the number of channels, upsampler blocks, and\n",
    "\t\t# decoder blocks\n",
    "\t\tself.channels = channels\n",
    "\t\tself.upconvs = ModuleList(\n",
    "\t\t\t[ConvTranspose2d(channels[i], channels[i + 1], 2, 2)\n",
    "\t\t\t \tfor i in range(len(channels) - 1)])\n",
    "\t\tself.dec_blocks = ModuleList(\n",
    "\t\t\t[Block(channels[i], channels[i + 1])\n",
    "\t\t\t \tfor i in range(len(channels) - 1)])\n",
    "\tdef forward(self, x, encFeatures):\n",
    "\t\t# loop through the number of channels\n",
    "\t\tfor i in range(len(self.channels) - 1):\n",
    "\t\t\t# pass the inputs through the upsampler blocks\n",
    "\t\t\tx = self.upconvs[i](x)\n",
    "\t\t\t# crop the current features from the encoder blocks,\n",
    "\t\t\t# concatenate them with the current upsampled features,\n",
    "\t\t\t# and pass the concatenated output through the current\n",
    "\t\t\t# decoder block\n",
    "\t\t\tencFeat = self.crop(encFeatures[i], x)\n",
    "\t\t\tx = torch.cat([x, encFeat], dim=1)\n",
    "\t\t\tx = self.dec_blocks[i](x)\n",
    "\t\t# return the final decoder output\n",
    "\t\treturn x\n",
    "\tdef crop(self, encFeatures, x):\n",
    "\t\t# grab the dimensions of the inputs, and crop the encoder\n",
    "\t\t# features to match the dimensions\n",
    "\t\t(_, _, H, W) = x.shape\n",
    "\t\tencFeatures = CenterCrop([H, W])(encFeatures)\n",
    "\t\t# return the cropped features\n",
    "\t\treturn encFeatures\n",
    "\t\n",
    "class UNet(Module):\n",
    "\tdef __init__(self, encChannels=(3, 16, 32, 64),\n",
    "\t\t decChannels=(64, 32, 16),\n",
    "\t\t nbClasses=20, retainDim=True,\n",
    "\t\t outSize=(128,  128)):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# initialize the encoder and decoder\n",
    "\t\tself.encoder = Encoder(encChannels)\n",
    "\t\tself.decoder = Decoder(decChannels)\n",
    "\t\t# initialize the regression head and store the class variables\n",
    "\t\tself.head = Conv2d(decChannels[-1], nbClasses, 1)\n",
    "\t\tself.retainDim = retainDim\n",
    "\t\tself.outSize = outSize\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# grab the features from the encoder\n",
    "\t\tencFeatures = self.encoder(x)\n",
    "\t\t# pass the encoder features through decoder making sure that\n",
    "\t\t# their dimensions are suited for concatenation\n",
    "\t\tdecFeatures = self.decoder(encFeatures[::-1][0],\n",
    "\t\t\tencFeatures[::-1][1:])\n",
    "\t\t# pass the decoder features through the regression head to\n",
    "\t\t# obtain the segmentation mask\n",
    "\t\tmap = self.head(decFeatures)\n",
    "\t\t# check to see if we are retaining the original output\n",
    "\t\t# dimensions and if so, then resize the output to match them\n",
    "\t\tif self.retainDim:\n",
    "\t\t\tmap = F.interpolate(map, self.outSize)\n",
    "\t\t# return the segmentation map\n",
    "\t\treturn map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock_2(nn.Module):        \n",
    "    # Consists of Conv -> ReLU -> MaxPool\n",
    "    def __init__(self, in_chans, out_chans, layers=2, sampling_factor=2, padding=\"same\"):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.encoder.append(nn.Conv2d(in_chans, out_chans, 3, 1, padding=padding))\n",
    "        self.encoder.append(nn.ReLU())\n",
    "        for _ in range(layers-1):\n",
    "            self.encoder.append(nn.Conv2d(out_chans, out_chans, 3, 1, padding=padding))\n",
    "            self.encoder.append(nn.ReLU())\n",
    "        self.mp = nn.MaxPool2d(sampling_factor)\n",
    "    def forward(self, x):\n",
    "        for enc in self.encoder:\n",
    "            x = enc(x)\n",
    "        mp_out = self.mp(x)\n",
    "        return mp_out, x\n",
    "\n",
    "class DecoderBlock_2(nn.Module):\n",
    "    # Consists of 2x2 transposed convolution -> Conv -> relu\n",
    "    def __init__(self, in_chans, out_chans, layers=2, skip_connection=True, sampling_factor=2, padding=\"same\"):\n",
    "        super().__init__()\n",
    "        skip_factor = 1 if skip_connection else 2\n",
    "        self.decoder = nn.ModuleList()\n",
    "        self.tconv = nn.ConvTranspose2d(in_chans, in_chans//2, sampling_factor, sampling_factor)\n",
    "\n",
    "        self.decoder.append(nn.Conv2d(in_chans//skip_factor, out_chans, 3, 1, padding=padding))\n",
    "        self.decoder.append(nn.ReLU())\n",
    "\n",
    "        for _ in range(layers-1):\n",
    "            self.decoder.append(nn.Conv2d(out_chans, out_chans, 3, 1, padding=padding))\n",
    "            self.decoder.append(nn.ReLU())\n",
    "\n",
    "        self.skip_connection = skip_connection\n",
    "        self.padding = padding\n",
    "    def forward(self, x, enc_features=None):\n",
    "        x = self.tconv(x)\n",
    "        if self.skip_connection:\n",
    "            if self.padding != \"same\":\n",
    "                # Crop the enc_features to the same size as input\n",
    "                w = x.size(-1)\n",
    "                c = (enc_features.size(-1) - w) // 2\n",
    "                enc_features = enc_features[:,:,c:c+w,c:c+w]\n",
    "            x = torch.cat((enc_features, x), dim=1)\n",
    "        for dec in self.decoder:\n",
    "            x = dec(x)\n",
    "        return x\n",
    "\n",
    "class UNet_2(nn.Module):\n",
    "    def __init__(self, nclass=1, in_chans=1, depth=5, layers=2, sampling_factor=2, skip_connection=True, padding=\"same\"):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.decoder = nn.ModuleList()\n",
    "\n",
    "        out_chans = 64\n",
    "        for _ in range(depth):\n",
    "            self.encoder.append(EncoderBlock_2(in_chans, out_chans, layers, sampling_factor, padding))\n",
    "            in_chans, out_chans = out_chans, out_chans*2\n",
    "\n",
    "        out_chans = in_chans // 2\n",
    "        for _ in range(depth-1):\n",
    "            self.decoder.append(DecoderBlock_2(in_chans, out_chans, layers, skip_connection, sampling_factor, padding))\n",
    "            in_chans, out_chans = out_chans, out_chans//2\n",
    "        # Add a 1x1 convolution to produce final classes\n",
    "        self.logits = nn.Conv2d(in_chans, nclass, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = []\n",
    "        for enc in self.encoder:\n",
    "            x, enc_output = enc(x)\n",
    "            encoded.append(enc_output)\n",
    "        x = encoded.pop()\n",
    "        for dec in self.decoder:\n",
    "            enc_output = encoded.pop()\n",
    "            x = dec(x, enc_output)\n",
    "\n",
    "        # Return the logits\n",
    "        return self.logits(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet().to(device='cuda:0')\n",
    "# model = UNet_2(nclass=20, in_chans=3).to('cuda:0')\n",
    "# initialize loss function and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# initialize a dictionary to store training history\n",
    "H = {\"train_loss\": [], \"test_loss\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading patch metadata . . .\n",
      "Done.\n",
      "Dataset ready.\n",
      "Reading patch metadata . . .\n",
      "Done.\n",
      "Dataset ready.\n",
      "Reading patch metadata . . .\n",
      "Done.\n",
      "Dataset ready.\n",
      "Train 1455, Val 482, Test 496\n",
      "EPOCH 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\blake\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:775: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [2/7], Loss: 3.0187, mIoU 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m21\u001b[39m):\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, \u001b[38;5;241m20\u001b[39m))\n\u001b[1;32m---> 65\u001b[0m     train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43miterate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation . . . \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 38\u001b[0m, in \u001b[0;36miterate\u001b[1;34m(model, data_loader, criterion, optimizer, mode, device)\u001b[0m\n\u001b[0;32m     35\u001b[0m IoU \u001b[38;5;241m=\u001b[39m JaccardIndex(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m'\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     36\u001b[0m t_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m         batch \u001b[38;5;241m=\u001b[39m recursive_todevice(batch, device)\n",
      "File \u001b[1;32mc:\\Users\\blake\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\blake\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\blake\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\blake\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\blake\\OneDrive\\Desktop\\Computer Vision\\Project_repo\\pastis-benchmark\\code\\dataloader.py:176\u001b[0m, in \u001b[0;36mPASTIS_Dataset.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# Retrieve and prepare satellite data\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mor\u001b[39;00m item \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m--> 176\u001b[0m     data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    177\u001b[0m         satellite: np\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m    178\u001b[0m             os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    179\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfolder,\n\u001b[0;32m    180\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATA_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(satellite),\n\u001b[0;32m    181\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(satellite, id_patch),\n\u001b[0;32m    182\u001b[0m             )\n\u001b[0;32m    183\u001b[0m         )\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    184\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m satellite \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msats\n\u001b[0;32m    185\u001b[0m     }  \u001b[38;5;66;03m# T x C x H x W arrays\u001b[39;00m\n\u001b[0;32m    186\u001b[0m     data \u001b[38;5;241m=\u001b[39m {s: torch\u001b[38;5;241m.\u001b[39mfrom_numpy(a) \u001b[38;5;28;01mfor\u001b[39;00m s, a \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\blake\\OneDrive\\Desktop\\Computer Vision\\Project_repo\\pastis-benchmark\\code\\dataloader.py:177\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# Retrieve and prepare satellite data\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mor\u001b[39;00m item \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    176\u001b[0m     data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m--> 177\u001b[0m         satellite: \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDATA_\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43msatellite\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43msatellite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_patch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    184\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m satellite \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msats\n\u001b[0;32m    185\u001b[0m     }  \u001b[38;5;66;03m# T x C x H x W arrays\u001b[39;00m\n\u001b[0;32m    186\u001b[0m     data \u001b[38;5;241m=\u001b[39m {s: torch\u001b[38;5;241m.\u001b[39mfrom_numpy(a) \u001b[38;5;28;01mfor\u001b[39;00m s, a \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\blake\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\npyio.py:432\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[0;32m    430\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 432\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[1;32mc:\\Users\\blake\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\format.py:790\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;66;03m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    792\u001b[0m         \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[0;32m    793\u001b[0m         \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    801\u001b[0m         \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[0;32m    802\u001b[0m         \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[0;32m    803\u001b[0m         array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mndarray(count, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fold_sequence = [\n",
    "    [[1, 2, 3], [4], [5]],\n",
    "    [[2, 3, 4], [5], [1]],\n",
    "    [[3, 4, 5], [1], [2]],\n",
    "    [[4, 5, 1], [2], [3]],\n",
    "    [[5, 1, 2], [3], [4]],\n",
    "]\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "prepare_output()\n",
    "device = torch.device('cuda:0')\n",
    "batch_size = 200\n",
    "\n",
    "for fold, (train_folds, val_fold, test_fold) in enumerate(fold_sequence):\n",
    "    # Dataset definition\n",
    "    dt_args = dict(\n",
    "        folder='D:/PASTIS',\n",
    "        norm=True,\n",
    "        target=\"semantic\",\n",
    "        sats=[\"S2\"],\n",
    "    )\n",
    "\n",
    "    dt_train = PASTIS_Dataset(**dt_args, folds=train_folds)\n",
    "    dt_val = PASTIS_Dataset(**dt_args, folds=val_fold)\n",
    "    dt_test = PASTIS_Dataset(**dt_args, folds=test_fold)\n",
    "\n",
    "    collate_fn = pad_collate\n",
    "    train_loader = data.DataLoader(\n",
    "        dt_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    val_loader = data.DataLoader(\n",
    "        dt_val,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    test_loader = data.DataLoader(\n",
    "        dt_test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"Train {}, Val {}, Test {}\".format(len(dt_train), len(dt_val), len(dt_test))\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    trainlog = {}\n",
    "    best_mIoU = 0\n",
    "\n",
    "    weights = torch.ones(20, device=device).float()\n",
    "    criterion = CrossEntropyLoss(weight=weights)\n",
    "\n",
    "    for epoch in range(1, 21):\n",
    "        print(\"EPOCH {}/{}\".format(epoch, 20))\n",
    "\n",
    "        train_metrics = iterate(\n",
    "            model,\n",
    "            data_loader=train_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            mode=\"train\",\n",
    "            device=device,\n",
    "        )\n",
    "        if epoch % 4 == 0 and epoch > 3:\n",
    "            print(\"Validation . . . \")\n",
    "            model.eval()\n",
    "            val_metrics = iterate(\n",
    "                model,\n",
    "                data_loader=val_loader,\n",
    "                criterion=criterion,\n",
    "                optimizer=optimizer,\n",
    "                mode=\"val\",\n",
    "                device=device,\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                \"Loss {:.4f}, IoU {:.4f}\".format(\n",
    "                    val_metrics[\"val_loss\"],\n",
    "                    val_metrics[\"val_IoU\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "            trainlog[epoch] = {**train_metrics, **val_metrics}\n",
    "            checkpoint(fold + 1, trainlog)\n",
    "            if val_metrics[\"val_IoU\"] >= best_mIoU:\n",
    "                best_mIoU = val_metrics[\"val_IoU\"]\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"state_dict\": model.state_dict(),\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                    },\n",
    "                    os.path.join(\n",
    "                        'output', \"Fold_{}\".format(fold + 1), \"model2.pth.tar\"\n",
    "                    ),\n",
    "                )\n",
    "        else:\n",
    "            trainlog[epoch] = {**train_metrics}\n",
    "            checkpoint(fold + 1, trainlog)\n",
    "\n",
    "    print(\"Testing best epoch . . .\")\n",
    "    model.load_state_dict(\n",
    "        torch.load(\n",
    "            os.path.join(\n",
    "                'output', \"Fold_{}\".format(fold + 1), \"model2.pth.tar\"\n",
    "            )\n",
    "        )[\"state_dict\"]\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    test_metrics = iterate(\n",
    "        model,\n",
    "        data_loader=test_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        mode=\"test\",\n",
    "        device=device,\n",
    "    )\n",
    "    print(\n",
    "        \"Loss {:.4f},  IoU {:.4f}\".format(\n",
    "            test_metrics[\"test_loss\"],\n",
    "            test_metrics[\"test_IoU\"]\n",
    "        )\n",
    "    )\n",
    "    save_results(fold + 1, test_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
